# LiveKit Inference LLM Models - Pricing & Metadata
# Prices are in USD per 1M tokens
# Source: https://livekit.io/pricing/inference

last_updated: "2026-01-28"

models:
  # =============================================================================
  # OpenAI Models
  # =============================================================================
  
  - model_id: "openai/gpt-4o"
    display_name: "GPT-4o"
    provider: "openai"
    context_window: 128000
    max_output: 16384
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 2.500
        cached_per_1m: 1.250
        output_per_1m: 10.000
      openai:
        input_per_1m: 2.500
        cached_per_1m: 1.250
        output_per_1m: 10.000

  - model_id: "openai/gpt-4o-mini"
    display_name: "GPT-4o Mini"
    provider: "openai"
    context_window: 128000
    max_output: 16384
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      azure:
        input_per_1m: 0.150
        cached_per_1m: 0.075
        output_per_1m: 0.600
      openai:
        input_per_1m: 0.150
        cached_per_1m: 0.075
        output_per_1m: 0.600

  - model_id: "openai/gpt-4.1"
    display_name: "GPT-4.1"
    provider: "openai"
    context_window: 1000000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 2.000
        cached_per_1m: 0.500
        output_per_1m: 8.000
      openai:
        input_per_1m: 2.000
        cached_per_1m: 0.500
        output_per_1m: 8.000

  - model_id: "openai/gpt-4.1-mini"
    display_name: "GPT-4.1 Mini"
    provider: "openai"
    context_window: 1000000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "standard"
    providers:
      azure:
        input_per_1m: 0.400
        cached_per_1m: 0.100
        output_per_1m: 1.600
      openai:
        input_per_1m: 0.400
        cached_per_1m: 0.100
        output_per_1m: 1.600

  - model_id: "openai/gpt-4.1-nano"
    display_name: "GPT-4.1 Nano"
    provider: "openai"
    context_window: 1000000
    max_output: 32768
    capabilities: ["function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      azure:
        input_per_1m: 0.100
        cached_per_1m: 0.030
        output_per_1m: 0.400
      openai:
        input_per_1m: 0.100
        cached_per_1m: 0.025
        output_per_1m: 0.400

  - model_id: "openai/gpt-5"
    display_name: "GPT-5"
    provider: "openai"
    context_window: 256000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 1.250
        cached_per_1m: 0.130
        output_per_1m: 10.000
      openai:
        input_per_1m: 1.250
        cached_per_1m: 0.125
        output_per_1m: 10.000

  - model_id: "openai/gpt-5-mini"
    display_name: "GPT-5 Mini"
    provider: "openai"
    context_window: 256000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "standard"
    providers:
      azure:
        input_per_1m: 0.250
        cached_per_1m: 0.030
        output_per_1m: 2.000
      openai:
        input_per_1m: 0.250
        cached_per_1m: 0.025
        output_per_1m: 2.000

  - model_id: "openai/gpt-5-nano"
    display_name: "GPT-5 Nano"
    provider: "openai"
    context_window: 128000
    max_output: 16384
    capabilities: ["function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      azure:
        input_per_1m: 0.050
        cached_per_1m: 0.010
        output_per_1m: 0.400
      openai:
        input_per_1m: 0.050
        cached_per_1m: 0.005
        output_per_1m: 0.400

  - model_id: "openai/gpt-5.1"
    display_name: "GPT-5.1"
    provider: "openai"
    context_window: 256000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 1.250
        cached_per_1m: 0.130
        output_per_1m: 10.000
      openai:
        input_per_1m: 1.250
        cached_per_1m: 0.125
        output_per_1m: 10.000

  - model_id: "openai/gpt-5.1-chat-latest"
    display_name: "GPT-5.1 Chat Latest"
    provider: "openai"
    context_window: 256000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 1.250
        cached_per_1m: 0.130
        output_per_1m: 10.000
      openai:
        input_per_1m: 1.250
        cached_per_1m: 0.125
        output_per_1m: 10.000

  - model_id: "openai/gpt-5.2"
    display_name: "GPT-5.2"
    provider: "openai"
    context_window: 256000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 1.750
        cached_per_1m: 0.180
        output_per_1m: 14.000
      openai:
        input_per_1m: 1.750
        cached_per_1m: 0.175
        output_per_1m: 14.000

  - model_id: "openai/gpt-5.2-chat-latest"
    display_name: "GPT-5.2 Chat Latest"
    provider: "openai"
    context_window: 256000
    max_output: 32768
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      azure:
        input_per_1m: 1.750
        cached_per_1m: 0.180
        output_per_1m: 14.000
      openai:
        input_per_1m: 1.750
        cached_per_1m: 0.175
        output_per_1m: 14.000

  - model_id: "openai/gpt-oss-120b"
    display_name: "GPT OSS 120B"
    provider: "openai"
    context_window: 128000
    max_output: 8192
    capabilities: ["function_calling", "streaming"]
    speed: "fast"
    tier: "budget"
    description: "Open-source 120B parameter model"
    providers:
      baseten:
        input_per_1m: 0.100
        cached_per_1m: null
        output_per_1m: 0.500
      groq:
        input_per_1m: 0.150
        cached_per_1m: 0.075
        output_per_1m: 0.600
      # cerebras:  # Coming soon
      #   input_per_1m: 0.350
      #   cached_per_1m: null
      #   output_per_1m: 0.750

  # =============================================================================
  # Google Gemini Models
  # =============================================================================

  - model_id: "google/gemini-3-pro"
    display_name: "Gemini 3 Pro"
    provider: "google"
    context_window: 2000000
    max_output: 65536
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    description: "Preview model"
    providers:
      google:
        input_per_1m: 4.000
        cached_per_1m: 0.400
        output_per_1m: 18.000

  - model_id: "google/gemini-3-flash"
    display_name: "Gemini 3 Flash"
    provider: "google"
    context_window: 1000000
    max_output: 65536
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "standard"
    description: "Preview model"
    providers:
      google:
        input_per_1m: 0.500
        cached_per_1m: 0.050
        output_per_1m: 3.000

  - model_id: "google/gemini-2.5-pro"
    display_name: "Gemini 2.5 Pro"
    provider: "google"
    context_window: 1000000
    max_output: 65536
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "standard"
    tier: "flagship"
    providers:
      google:
        input_per_1m: 2.500
        cached_per_1m: 0.250
        output_per_1m: 15.000

  - model_id: "google/gemini-2.5-flash"
    display_name: "Gemini 2.5 Flash"
    provider: "google"
    context_window: 1000000
    max_output: 65536
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "standard"
    providers:
      google:
        input_per_1m: 0.300
        cached_per_1m: 0.030
        output_per_1m: 2.500

  - model_id: "google/gemini-2.5-flash-lite"
    display_name: "Gemini 2.5 Flash Lite"
    provider: "google"
    context_window: 1000000
    max_output: 65536
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      google:
        input_per_1m: 0.100
        cached_per_1m: 0.010
        output_per_1m: 0.400

  - model_id: "google/gemini-2.0-flash"
    display_name: "Gemini 2.0 Flash"
    provider: "google"
    context_window: 1000000
    max_output: 8192
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "standard"
    providers:
      google:
        input_per_1m: 0.100
        cached_per_1m: null
        output_per_1m: 0.400

  - model_id: "google/gemini-2.0-flash-lite"
    display_name: "Gemini 2.0 Flash Lite"
    provider: "google"
    context_window: 1000000
    max_output: 8192
    capabilities: ["vision", "function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      google:
        input_per_1m: 0.075
        cached_per_1m: null
        output_per_1m: 0.300

  # =============================================================================
  # Kimi Models
  # =============================================================================

  - model_id: "moonshotai/kimi-k2-instruct"
    display_name: "Kimi K2 Instruct"
    provider: "moonshot"
    context_window: 128000
    max_output: 8192
    capabilities: ["function_calling", "streaming"]
    speed: "fast"
    tier: "standard"
    providers:
      baseten:
        input_per_1m: 0.600
        cached_per_1m: null
        output_per_1m: 2.500

  # =============================================================================
  # DeepSeek Models
  # =============================================================================

  - model_id: "deepseek-ai/deepseek-v3"
    display_name: "DeepSeek V3"
    provider: "deepseek"
    context_window: 128000
    max_output: 8192
    capabilities: ["function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      baseten:
        input_per_1m: 0.770
        cached_per_1m: null
        output_per_1m: 0.770

  - model_id: "deepseek-ai/deepseek-v3.2"
    display_name: "DeepSeek V3.2"
    provider: "deepseek"
    context_window: 128000
    max_output: 8192
    capabilities: ["function_calling", "json_mode", "streaming"]
    speed: "fast"
    tier: "budget"
    providers:
      baseten:
        input_per_1m: 0.300
        cached_per_1m: null
        output_per_1m: 0.450
